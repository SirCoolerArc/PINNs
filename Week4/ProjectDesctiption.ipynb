{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b3c543-135c-4eee-abf0-e29d87643f9a",
   "metadata": {},
   "source": [
    "# The Cooling Coffee Cup: PINN\n",
    "This project focuses on modeling the temperature of a cooling coffee cup over time, governed by Newton's Law of Cooling. This involves an ordinary differential equation (ODE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d12df-47ad-41fa-a4d8-95b77017e3b9",
   "metadata": {},
   "source": [
    "## The Physics: Newton's Law of Cooling\n",
    "The fundamental physical principle at play is Newton's Law of Cooling, which states that the rate of heat loss of a body is directly proportional to the difference in temperatures between the body and its surroundings. Mathematically, this is expressed as an ordinary differential equation (ODE):\n",
    "$$\n",
    "\\frac{dT}{dt} = -r(T - T_{\\text{ambient}})\n",
    "$$\n",
    "Where:\n",
    "* T is the temperature of the coffee cup at time t.\n",
    "* t is time.\n",
    "* r is the cooling rate constant (a positive value).\n",
    "* T_ambient is the constant ambient temperature of the surroundings (eg: room temperature).\n",
    "\n",
    "The goal of the PINN in this project is to learn the temperature T(t) as a function of time t that satisfies this equation, potentially even inferring the unknown cooling rate r if it's not provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9515ec9d-6a29-428b-8974-e492c52311e3",
   "metadata": {},
   "source": [
    "## Python Libraries Used\n",
    "* PyTorch\n",
    "* NumPy\n",
    "* Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741edd57-c631-4bb0-b743-ed0ad109eb55",
   "metadata": {},
   "source": [
    "## Key Implementation Details using PyTorch\n",
    "\n",
    "The project leverages PyTorch's capabilities for building neural networks and, crucially, its automatic differentiation engine.\n",
    "\n",
    "* Neural Network Architecture:\n",
    "A simple feedforward neural network (often referred to as an MLP - Multi-Layer Perceptron) is used. <br>\n",
    "The input to the network is t (time). <br>\n",
    "The output of the network is T_pred (the predicted temperature at that time t). <br>\n",
    "The network typically consists of a few torch.nn.Linear layers with activation functions (e.g., tanh or sigmoid) to introduce non-linearity, followed by a final linear layer for the output.   \n",
    "\n",
    "* Loss Function:\n",
    "The power of PINNs lies in their custom loss function, which combines two main components: <br>\n",
    "### Data-Fitting Loss (MSE_data): \n",
    "This part of the loss measures how well the neural network's predictions match any available observed temperature data points. If you have actual measurements of the coffee cup's temperature at different times, this loss term ensures the network learns from that data. It's typically a Mean Squared Error (MSE) between the network's predicted temperature T_pred and the actual observed temperature T_obs at those data points. <p>\n",
    "\n",
    "### Physics-Informed Loss (MSE_physics):\n",
    "This is the core of the PINN. It enforces Newton's Law of Cooling.\n",
    "1. The neural network predicts T_pred for a given t.\n",
    "2. PyTorch's automatic differentiation (torch.autograd.grad) is used to compute the derivative of T_pred with respect to t (i.e., dT/dt).\n",
    "3. The physics-informed loss is then calculated as the Mean Squared Error of the residual of Newton's Law of Cooling. The network tries to make (dT/dt) - (-r(T_pred - T_ambient)) as close to zero as possible.   <p>\n",
    "\n",
    "### Total Loss:\n",
    "The total loss is the sum of the data-fitting loss and the physics-informed loss. The training process then minimizes this total loss.\n",
    "\n",
    "* Automatic Differentiation:\n",
    "PyTorch's torch.autograd.grad is fundamental here. It allows you to compute the derivatives of the neural network's output (T_pred) with respect to its input (t) without manually deriving the equations. This is how the dT/dt term for the physics-informed loss is obtained.   \n",
    "\n",
    "* Inferring Unknown Parameters:\n",
    "A particularly insightful aspect of this example is its ability to infer the cooling rate r. Instead of setting r as a fixed constant, it can be treated as a differentiable parameter within the PyTorch model. During training, the optimizer adjusts r along with the network's weights and biases to minimize the total loss, effectively discovering the cooling rate from the data and physics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc289d7-c7f3-484b-8f45-a1d5ae851b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
